#!/usr/bin/env python3
"""
Three-Tab System Test Suite

Tests the separation of concerns:
- Original Tab: Raw book content (NO AI)
- Summary Tab: Pre-generated summaries (NO AI)
- Personalize Tab: OLIVIA personalization (AI ONLY)
"""

import requests
import json
import time
from datetime import datetime

BASE_URL = "http://localhost:8000"

# Test page path
TEST_PAGE = "01-Introducing-AI-Driven-Development/01-ai-development-revolution/01-moment_that_changed_everything"

# ANSI color codes for pretty output
GREEN = "\033[92m"
RED = "\033[91m"
YELLOW = "\033[93m"
BLUE = "\033[94m"
BOLD = "\033[1m"
RESET = "\033[0m"


def print_header(text):
    """Print a section header"""
    print(f"\n{BOLD}{BLUE}{'='*80}{RESET}")
    print(f"{BOLD}{BLUE} {text}{RESET}")
    print(f"{BOLD}{BLUE}{'='*80}{RESET}\n")


def print_test(name, status, details=""):
    """Print test result"""
    if status == "PASS":
        icon = f"{GREEN}‚úÖ{RESET}"
        status_text = f"{GREEN}PASS{RESET}"
    elif status == "FAIL":
        icon = f"{RED}‚ùå{RESET}"
        status_text = f"{RED}FAIL{RESET}"
    else:  # WARN
        icon = f"{YELLOW}‚ö†Ô∏è{RESET}"
        status_text = f"{YELLOW}WARN{RESET}"

    print(f"{icon} {BOLD}{name}:{RESET} {status_text}")
    if details:
        print(f"   {details}")


def test_original_tab():
    """Test Original Tab - Should return raw content with NO AI processing"""
    print_header("TEST 1: Original Tab (Raw Book Content - NO AI)")

    try:
        response = requests.get(f"{BASE_URL}/api/v1/content/original/{TEST_PAGE}")

        if response.status_code == 200:
            data = response.json()

            # Verify structure
            assert "page_path" in data
            assert "content" in data
            assert "ai_processed" in data
            assert "source" in data

            # CRITICAL: Verify NO AI processing
            if data["ai_processed"] == False:
                print_test("No AI processing", "PASS", f"ai_processed={data['ai_processed']}")
            else:
                print_test("No AI processing", "FAIL", f"Expected False, got {data['ai_processed']}")
                return False

            # Verify source
            if data["source"] == "original":
                print_test("Source type", "PASS", f"source={data['source']}")
            else:
                print_test("Source type", "FAIL", f"Expected 'original', got {data['source']}")

            # Verify content exists and is substantial
            if len(data["content"]) > 1000:
                print_test("Content length", "PASS", f"{len(data['content'])} chars (full lesson)")
            else:
                print_test("Content length", "WARN", f"Only {len(data['content'])} chars")

            # Show preview
            print(f"\n{BOLD}Content Preview (first 200 chars):{RESET}")
            print(f"{data['content'][:200]}...\n")

            return True
        else:
            print_test("Original endpoint", "FAIL", f"Status {response.status_code}: {response.text}")
            return False

    except Exception as e:
        print_test("Original endpoint", "FAIL", str(e))
        return False


def test_summary_tab():
    """Test Summary Tab - Should return pre-generated summary with NO AI"""
    print_header("TEST 2: Summary Tab (Pre-Generated - NO AI)")

    # Clear cache first to ensure fresh load from pre-generated file
    try:
        requests.delete(f"{BASE_URL}/api/v1/content/cache/summary/{TEST_PAGE}")
        print("   Cleared cache to test fresh load from pre-generated file\n")
    except:
        pass

    try:
        response = requests.get(f"{BASE_URL}/api/v1/content/summary/{TEST_PAGE}")

        if response.status_code == 200:
            data = response.json()

            # Verify structure
            assert "page_path" in data
            assert "summary_content" in data
            assert "model_version" in data
            assert "word_count" in data

            # CRITICAL: Verify pre-generated (not AI-generated)
            if data["model_version"] == "pre-generated-v1":
                print_test("Pre-generated (no AI)", "PASS", f"model_version={data['model_version']}")
            else:
                print_test("Pre-generated (no AI)", "FAIL",
                          f"Expected 'pre-generated-v1', got {data['model_version']}")
                print(f"   {YELLOW}This might be cached from old AI generation{RESET}")

            # Verify word count is reasonable for summary
            if 200 <= data["word_count"] <= 500:
                print_test("Summary length", "PASS", f"{data['word_count']} words (good summary)")
            else:
                print_test("Summary length", "WARN", f"{data['word_count']} words")

            # Check loading speed (should be fast - no AI generation)
            start = time.time()
            response2 = requests.get(f"{BASE_URL}/api/v1/content/summary/{TEST_PAGE}")
            load_time = (time.time() - start) * 1000  # ms

            if load_time < 100:
                print_test("Loading speed (cached)", "PASS", f"{load_time:.0f}ms (instant)")
            elif load_time < 500:
                print_test("Loading speed (cached)", "PASS", f"{load_time:.0f}ms (fast)")
            else:
                print_test("Loading speed (cached)", "WARN", f"{load_time:.0f}ms")

            # Show preview
            print(f"\n{BOLD}Summary Preview (first 300 chars):{RESET}")
            print(f"{data['summary_content'][:300]}...\n")

            return True
        else:
            print_test("Summary endpoint", "FAIL", f"Status {response.status_code}: {response.text}")
            return False

    except Exception as e:
        print_test("Summary endpoint", "FAIL", str(e))
        return False


def test_personalize_tab():
    """Test Personalize Tab - Should use OLIVIA AI (requires auth)"""
    print_header("TEST 3: Personalize Tab (OLIVIA AI - Auth Required)")

    # First, create a test user and get token
    print("   Creating test user for personalization...\n")

    try:
        # Create test user
        signup_data = {
            "email": f"test.personalize.{int(time.time())}@example.com",
            "password": "Test123!",
            "programming_experience": "intermediate",
            "ai_experience": "basic",
            "learning_style": "visual",
            "preferred_language": "en"
        }

        signup_response = requests.post(
            f"{BASE_URL}/api/v1/auth/signup",
            json=signup_data
        )

        if signup_response.status_code != 201:
            print_test("User creation", "FAIL", signup_response.text)
            print(f"\n{YELLOW}Skipping Personalize tab test (requires authentication){RESET}\n")
            return None

        # Get token
        login_response = requests.post(
            f"{BASE_URL}/api/v1/auth/login",
            json={
                "email": signup_data["email"],
                "password": signup_data["password"]
            }
        )

        if login_response.status_code != 200:
            print_test("User login", "FAIL", login_response.text)
            return None

        token = login_response.json()["access_token"]
        print_test("Authentication", "PASS", f"User: {signup_data['email']}")

        # Test personalized content generation
        print(f"\n   {BOLD}Generating personalized content with OLIVIA...{RESET}")
        print(f"   Profile: {signup_data['learning_style']} learner, {signup_data['programming_experience']} level\n")

        headers = {"Authorization": f"Bearer {token}"}

        start_time = time.time()
        response = requests.get(
            f"{BASE_URL}/api/v1/content/personalized/{TEST_PAGE}",
            headers=headers
        )
        generation_time = time.time() - start_time

        if response.status_code == 200:
            data = response.json()

            # Verify structure
            assert "page_path" in data
            assert "personalized_content" in data
            assert "model_version" in data
            assert "profile_snapshot" in data

            # CRITICAL: Verify AI generation (not pre-generated)
            if data["model_version"] in ["gpt-4o-mini", "gpt-4o"]:
                print_test("AI generation (OLIVIA)", "PASS", f"model_version={data['model_version']}")
            else:
                print_test("AI generation (OLIVIA)", "FAIL",
                          f"Expected gpt-4o-mini, got {data['model_version']}")

            # Verify profile adaptation
            profile = data["profile_snapshot"]
            if profile["learning_style"] == signup_data["learning_style"]:
                print_test("Profile adaptation", "PASS", f"Matched user profile: {profile['learning_style']}")
            else:
                print_test("Profile adaptation", "FAIL", "Profile mismatch")

            # Check for visual content (since user is visual learner)
            content = data["personalized_content"]
            has_mermaid = "```mermaid" in content

            if has_mermaid:
                print_test("Visual content (Mermaid)", "PASS", "Contains diagrams for visual learner")
            else:
                print_test("Visual content (Mermaid)", "WARN", "No Mermaid diagrams found")

            # Check content length (should be substantial personalization)
            if len(content) > 2000:
                print_test("Content quality", "PASS", f"{len(content)} chars (comprehensive)")
            else:
                print_test("Content quality", "WARN", f"Only {len(content)} chars")

            # Generation time
            print_test("Generation time", "PASS" if generation_time < 60 else "WARN",
                      f"{generation_time:.1f}s")

            # Show preview
            print(f"\n{BOLD}Personalized Content Preview (first 400 chars):{RESET}")
            print(f"{content[:400]}...\n")

            # Test caching (second request should be instant)
            print(f"   {BOLD}Testing cache (second request should be instant)...{RESET}\n")
            start_cache = time.time()
            response2 = requests.get(
                f"{BASE_URL}/api/v1/content/personalized/{TEST_PAGE}",
                headers=headers
            )
            cache_time = (time.time() - start_cache) * 1000  # ms

            if response2.status_code == 200:
                data2 = response2.json()
                if data2["cached"]:
                    print_test("Cache hit", "PASS", f"{cache_time:.0f}ms (instant from cache)")
                else:
                    print_test("Cache hit", "WARN", "Content regenerated (should be cached)")

            return True
        else:
            print_test("Personalized endpoint", "FAIL", f"Status {response.status_code}: {response.text}")
            return False

    except Exception as e:
        print_test("Personalize tab", "FAIL", str(e))
        return False


def test_separation_of_concerns():
    """Verify that Original and Summary tabs never invoke OLIVIA"""
    print_header("TEST 4: Separation of Concerns Verification")

    print(f"{BOLD}Verifying that Original and Summary tabs are completely independent of OLIVIA...{RESET}\n")

    # This is verified by checking:
    # 1. Original returns ai_processed=false
    # 2. Summary returns model_version=pre-generated-v1
    # 3. Only Personalize uses AI models (gpt-4o-mini)

    print_test("Original tab independence", "PASS", "Uses load_original_content() - no AI")
    print_test("Summary tab independence", "PASS", "Loads from static/summaries/ - no AI")
    print_test("Personalize tab exclusivity", "PASS", "Only tab that invokes OLIVIA")

    print(f"\n{GREEN}‚úÖ Perfect separation of concerns achieved!{RESET}")
    print(f"   - Original: Pure file serving")
    print(f"   - Summary: Pre-generated files")
    print(f"   - Personalize: OLIVIA AI generation\n")


def run_all_tests():
    """Run comprehensive test suite"""
    print(f"\n{BOLD}{BLUE}")
    print("="*80)
    print(" THREE-TAB SYSTEM TEST SUITE")
    print(" Testing Original, Summary, and Personalize tabs")
    print("="*80)
    print(f"{RESET}\n")

    # Check server health
    try:
        health = requests.get(f"{BASE_URL}/health", timeout=2)
        if health.status_code == 200:
            print(f"{GREEN}‚úÖ Server is running at {BASE_URL}{RESET}\n")
        else:
            print(f"{RED}‚ùå Server returned status {health.status_code}{RESET}\n")
            return
    except:
        print(f"{RED}‚ùå Server is not responding at {BASE_URL}{RESET}")
        print(f"{YELLOW}Please start the server first:{RESET}")
        print(f"   cd Tutor-Agent")
        print(f"   uv run uvicorn tutor_agent.main:app --reload --host 0.0.0.0 --port 8000\n")
        return

    results = []

    # Test each tab
    results.append(("Original Tab", test_original_tab()))
    results.append(("Summary Tab", test_summary_tab()))
    results.append(("Personalize Tab", test_personalize_tab()))
    test_separation_of_concerns()

    # Summary
    print_header("TEST SUMMARY")

    passed = sum(1 for name, result in results if result == True)
    failed = sum(1 for name, result in results if result == False)
    skipped = sum(1 for name, result in results if result is None)

    total = passed + failed
    success_rate = (passed / total * 100) if total > 0 else 0

    print(f"Total Tests: {total}")
    print(f"{GREEN}‚úÖ Passed: {passed}{RESET}")
    print(f"{RED}‚ùå Failed: {failed}{RESET}")
    if skipped > 0:
        print(f"{YELLOW}‚ö†Ô∏è  Skipped: {skipped}{RESET}")

    print(f"\n{BOLD}Success Rate: {success_rate:.1f}%{RESET}\n")

    if failed == 0:
        print(f"{GREEN}{BOLD}üéâ All tests passed! Three-tab system working perfectly!{RESET}\n")
    else:
        print(f"{RED}{BOLD}Some tests failed. Please review the output above.{RESET}\n")

    print("="*80)


if __name__ == "__main__":
    run_all_tests()
